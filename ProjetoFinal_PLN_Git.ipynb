{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tA0NzFEz0W18"
      },
      "source": [
        "# **Projeto final da Disciplina de Processamento de Linguagem Natural: Assistente Virtual do SEI Julgar**\n",
        "\n",
        "Bem-vindo ao notebook de desenvolvimento do **Assistente Virtual do SEI Julgar**!\n",
        "\n",
        "O SEI Julgar é um módulo do Sistema Eletrônico de Informações (SEI) que automatiza e gerencia o processo de julgamento de processos administrativos que precisam ir ao colegiado para serem votados.\n",
        "\n",
        "Este projeto demonstra como construir um chatbot de **Recuperação Aumentada por Geração (RAG)**, especializado em fornecer suporte técnico e instruções detalhadas sobre o uso do módulo **SEI Julgar**.\n",
        "\n",
        "**Projeto desenvolvido por:**\n",
        "\n",
        "André Cacau\n",
        "\n",
        "Vinícius de Souza Sá\n",
        "\n",
        "---\n",
        "\n",
        "**Problemas e Justificativa do Trabalho**\n",
        "\n",
        "Produzimos um manual sobre o módulo SEI Julgar, baseado em manuais de outros órgãos e de documentações oficiais do módulo. Ficou bem extenso e detalhado, o que poderia causar relutância dos usuários em utilizá-lo para dirimir dúvidas ou até mesmo para o entendimento de como executar algum procedimento no sistema. Logo, o chatbot surge para possibilitar um acesso rápido, dinâmico e intuitivo às informações por meio de conversas em linguagem natural.\n",
        "\n",
        "---\n",
        "**Objetivo do trabalho**\n",
        "\n",
        "Chatbot com RAG (*Retrieval-Augmented Generation*) que responde com base no manual do SEI Julgar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2hK_iS52Qxe"
      },
      "source": [
        "### **Instalação das bibliotecas**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpjxKLM40Ti4"
      },
      "outputs": [],
      "source": [
        "!pip install -q \\\n",
        "    langchain langchain-groq langchain_community langchain-huggingface \\\n",
        "    faiss-cpu chromadb PyPDF2 streamlit python-dotenv >/dev/null 2>&1\n",
        "\n",
        "!npm install -g localtunnel >/dev/null 2>&1\n",
        "\n",
        "!python -m spacy download pt_core_news_md --quiet >/dev/null 2>&1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfTlnkGc47tl"
      },
      "source": [
        "### **Importe o manual Documentação SEI Julgar - Secretaria.pdf**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "i_xBjROGveeP",
        "outputId": "56c35180-dd9b-4c4d-df95-2282bf3e037b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-50a59d4e-8fed-4f54-847a-bf42ffaac35c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-50a59d4e-8fed-4f54-847a-bf42ffaac35c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Documentação SEI Julgar - Secretaria.pdf to Documentação SEI Julgar - Secretaria.pdf\n",
            "Arquivo enviado: Documentação SEI Julgar - Secretaria.pdf\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Arquivo enviado: {filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5uK-6VV9O-m"
      },
      "source": [
        "---\n",
        "## **Configuração do Large Language Model (LLM)**\n",
        "Para iniciar o desenvolvimento do chatbot com RAG, definimos e inicializamos o LLM que será responsável por gerar as respostas finais para o usuário.\n",
        "\n",
        "Neste projeto, utilizamos a Groq Cloud, que se destaca pela sua velocidade de inferência e por ser gratuito.\n",
        "\n",
        "**Você irá precisa de uma chave Groq para realizar os testes**\n",
        "\n",
        "Acesse o site oficial:\n",
        "https://console.groq.com\n",
        "\n",
        "1. Faça login (Google ou GitHub).\n",
        "\n",
        "2. No menu lateral esquerdo, clique em API Keys.\n",
        "\n",
        "3. Clique no botão:\n",
        "“Create API Key” ou “Generate New Key”\n",
        "\n",
        "4. Copie a chave gerada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0Ya9YdT9qpE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "id_model = \"llama-3.3-70b-versatile\"\n",
        "temperature = 0.7\n",
        "os.environ[\"GROQ_API_KEY\"] = \"coloque sua chave aqui\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjCWLPdG-NKt"
      },
      "source": [
        "### **Carregar o LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YlG6nyUZ-SEb",
        "outputId": "5fac1a2d-b3ca-4375-c5c6-0a88612d4f46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Resposta do modelo:\n",
            "O SEI Julgar é um sistema eletrônico do Superior Tribunal de Justiça (STJ) que permite a apresentação de recursos e petições eletronicamente, facilitando o acesso à justiça e agilizando os processos judiciais. Ele é uma ferramenta importante para a modernização da justiça no Brasil.\n"
          ]
        }
      ],
      "source": [
        "#Cria a instância do modelo LLM que vai gerar as respostas do chatbot.\n",
        "def load_llm():\n",
        "    return ChatGroq(\n",
        "        model=id_model,\n",
        "        temperature=temperature,\n",
        "        api_key=os.getenv(\"GROQ_API_KEY\")\n",
        "    )\n",
        "\n",
        "llm = load_llm()\n",
        "\n",
        "# Teste prático mostrando o funcionamento da llm\n",
        "llm = load_llm()\n",
        "resposta = llm.invoke(\"Explique brevemente o que é o SEI Julgar.\")\n",
        "print(\" Resposta do modelo:\")\n",
        "print(resposta.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rO6I1VcQ-VqU"
      },
      "source": [
        " O teste anterior mostrou que, embora o LLM consiga gerar respostas em linguagem natural, ele não possui conhecimento específico sobre o módulo SEI Julgar.\n",
        "Isso ocorre porque o modelo foi treinado em um conjunto amplo de textos da internet. Logo:\n",
        "\n",
        "\n",
        "*   O LLM pode inventar (alucinar).\n",
        "*   Não tem acesso aos documentos do SEI Julgar.\n",
        "*   Não consegue responder com precisão perguntas específicas.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48xHLjmWfHsP"
      },
      "source": [
        "---\n",
        "## **Construção do RAG (Recuperação e Geração)**\n",
        "\n",
        "Como falamos anteriormente, por padrão o LLM não possui conhecimento do documento do SEI Julgar. Por conta disso, nesse momento que entra o RAG (Retrieval-Augmented Generation).\n",
        "\n",
        "De forma simples, a LLM busca informações relevantes em uma base de conhecimento antes de formular a resposta.\n",
        "\n",
        "O RAG funciona em duas grandes etapas:\n",
        "\n",
        "\n",
        "1.   Recuperação (Retriever)\n",
        "2.   Geração (RAG Chain)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jMw2-Tqnvkl"
      },
      "source": [
        "---\n",
        "## **Recuperação (Retriever)**\n",
        "Antes de responder qualquer pergunta, o sistema busca no banco vetorial os trechos mais relevantes do manual.\n",
        "\n",
        "Essa etapa garante que o modelo receba apenas informações diretamente relacionadas ao SEI Julgar.\n",
        "\n",
        "Para construir o retriver, iremos prosseguir com as próximas etapas:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwWg2mlHsuMp"
      },
      "source": [
        "### 1. Leitura e preparação dos documentos\n",
        "Nesta etapa, realizamos a leitura do PDF e extraímos o texto de cada página."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EesDinmUtPMl",
        "outputId": "fc01ad9e-a158-411d-fa7c-856f55399cbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Páginas lidas: 45\n",
            "2.6 - Registrar Presença na Sessão O SEI Julgar permite registrar se cada membro do colegiado está presente ou ausente na Sessão. Esse registro pode ser feito de duas maneiras: ● Registro em grupo: ○ Marque os membros do colegiado utilizando as caixas de seleção (checkbox) na lista. ○ Clique no botão Registrar Presença para confirmar a presença dos selecionados. ○ A presença tem as seguintes modalidades: Presencial ou Remoto ● Registro individual: ○ Utilize o ícone disponível na coluna Ações da tabela Membros do Colegiado para registrar a presença de cada membro separadamente. 9\n"
          ]
        }
      ],
      "source": [
        "from PyPDF2 import PdfReader\n",
        "\n",
        "pdf_file = \"Documentação SEI Julgar - Secretaria.pdf\"\n",
        "reader = PdfReader(pdf_file)\n",
        "\n",
        "texts = []\n",
        "\n",
        "for page in reader.pages:\n",
        "    raw = page.extract_text()\n",
        "    if raw:\n",
        "        # Remover quebras de linha e múltiplos espaços\n",
        "        clean = raw.replace(\"\\n\", \" \")\n",
        "        clean = \" \".join(clean.split())\n",
        "        texts.append(clean)\n",
        "\n",
        "print(\"Páginas lidas:\", len(texts))\n",
        "clean_text = texts[10].replace(\"\\n\", \" \") # Ler a página 10\n",
        "clean_text = \" \".join(clean_text.split())\n",
        "print(clean_text[:1000])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9srHVVGtYOK"
      },
      "source": [
        "### 2. Chunking (Segmentação do texto)\n",
        "Dividimos o texto em partes menores, para que a busca vetorial funcione com maior precisão."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dteEbM0OtnaE",
        "outputId": "bc388aca-0836-4cc4-8c8b-7de1a846b44e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total de chunks: 99\n",
            "2. SECRETARIA DO COLEGIADO 2.1 - Criar Sessão de Julgamento No menu lateral do SEI, acesse a opção “Sessões de Julgamento”. Nessa página estarão disponíveis os botões Pesquisar , Nova e Excluir . Para cadastrar uma nova sessão: 1. Clique em “Nova”; 2. Selecione o colegiado responsável; 3. Defina a d\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.create_documents(texts)\n",
        "\n",
        "print(\"Total de chunks:\", len(chunks))\n",
        "print(chunks[9].page_content[:300])\n",
        "# Como o pdf vem com muitas quebras de linhas o PyPDF2 coloca \\n em muita coisa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wz_OZCi2ttgX"
      },
      "source": [
        "### 3. Criação dos embeddings\n",
        "Cada chunk é convertido em um vetor numérico que representa seu significado semântico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510,
          "referenced_widgets": [
            "f763040c478a4a3f9ed428fd4552f0a4",
            "3256b51ff8ab47dba6408fa413dfd650",
            "1ea31b8dcaa3403689679e3d47a2a90d",
            "527ff51313b2489a872a499c62ac4c16",
            "c23c255579fe48efa18b57f8b70c11a7",
            "7102c7dfcb2043018e9daf07ea811e48",
            "2d113db2328e4d5196822e6382038e10",
            "481ccc1eb5894b94a62d9e0cb48bdd76",
            "c9ff464fee4f490e9bc584939d3aa8c3",
            "3f812bd82e0b4b5fa0655f4262996f9f",
            "931d1f9db2fb4de49b66eb172dd706d1",
            "68148838f0dd481baa7271e576deade1",
            "3b643ae16df2441e90ca09a07a0817d3",
            "22e7255ae5dd4c4d9bf874db764b750a",
            "742e756d20e149bea39d71c53e258e85",
            "8b2ecd884a9c475281082592c05087a6",
            "1dfb9287e6e2432099ebe0841c00509e",
            "770e356637114032a6ef8b10ae19693e",
            "1fdc55d786f6450f8aa7528645edf26b",
            "abcf00dd1c494fee87189f56f8b172e9",
            "3934dde40f294d61af8a447a77b74950",
            "cb9a3c80e09846fd8e6128b0c7652aa3",
            "6a951747edfb4f50aa8f7c067dc922b8",
            "c53f8f9da4ff4eff949ba8149c7b12a6",
            "a773396d2c0b46fa8623e56200a11f9e",
            "d44fedc763f84d1f9cd7bc9df9398a3f",
            "d697d29fc0c2466b9d55561f41429566",
            "a38e657ac3594ec49258f3d640725095",
            "ab8f7a5c12dc436e80bd622ee8df3e70",
            "c1c171ede54d4e95b8397353cfc4dbd1",
            "6797ff2307a54fe0bdc27c639e8d9368",
            "fc4934b500f14b34b45b6389ba87d878",
            "ce4a8c3b060644fbbd822196f64005da",
            "80a15c461e6644008a0cb803d4362441",
            "0f9426707f85487195e671cde4c5c34b",
            "79b777cc2bb14679a29f9ef7f7e50ff4",
            "4afaa51903994a5cae8847a4957f47f9",
            "a7d60412350c481cad4b9127f09f37f3",
            "8cfcb267a4d2407b8fd8b32a2b9ac12e",
            "1aade85c9e064b71a202a9f0f5c83d64",
            "b4149aec09f749028b2c9db90d3082c0",
            "f4d4058725b54f039878617740e8cbbe",
            "af961516c96146c58e3ff0ca653c5639",
            "1b716860779d4eef8029c8b059420b32",
            "149b0c1e07774884beb1f7fd1de0c9ca",
            "89b29c7c02324cf298e991cf32e0702e",
            "c4d71f833b31444f88baca89d81c4363",
            "6e8746b6120e40b2afd99dc0b2ccd73d",
            "da4a5096408148f79f35396aeebcb7ef",
            "c2eed1a4bd6c4a15995c2da2814de096",
            "21c9c53a0c3c461fa81ed03b99443990",
            "82b716ec67ac48c8a56c8164373e7b43",
            "3301e03e04634150be8143690bd9ec3f",
            "11bddfb98d01413a9444c980cb3692e6",
            "bc965c31f6d24e26975c17ac87c2a276",
            "ce767e803ace40d3b0ba36aa05437389",
            "b31ee2c674894482b03b6a4d1ca6bb3b",
            "1e7a3260d37b4b16aedb62daacc11489",
            "d1670214acf245b096b38809352885dd",
            "a615d2dd8249461e8b968eec2d60df30",
            "0329eae32f444660941c46ac2784ae22",
            "76cff6fce8f74f36909da4dba9983548",
            "b024ed34f5d64ec89b9bd4052a072190",
            "4688bf82705a4ec1886977e8ca2e5219",
            "64c12d84827249978214ea97185d156e",
            "a02cc4dafb514237a613ac11f2f1dadb",
            "eba76aee53614c53ac2677cba563b92b",
            "066b6a54df304f6dbf72c6420a69c6a7",
            "7c304ace82ce446d9fb9a477b4b8f3f8",
            "f538868075fa476db9b4fd0a225983af",
            "c9c8ef4117dd49e19d343bed71fd1833",
            "72e1cb88644d476fa1927069306982f1",
            "3f1e7757553b4d8fa4aafef52056dcf6",
            "35c4593761b64b428264140a211a69f3",
            "f6928811b3ce4d8995d53077e0f6a9b5",
            "9ebe48f6465247ae9a9cdc9385a09853",
            "aedac56988bd48259afbc2aa4eb9906c",
            "85acd388aae54b1691ceafa334dc6bc6",
            "7466f679c712441a8c757b186a9ac3ce",
            "1d3cc6f4fa6a4ffea24274dc547250a3",
            "7bd04ca999324dfab5d780db5b38c422",
            "d7fd9b3d9dff4cdaba26282412ac27f2",
            "310852d9ebf64d578d1d998d76ee94b1",
            "094b5f61e3c545eeadd88e4403311daa",
            "a211a4ae4ef5403da43dd48ec85c54f5",
            "515de8ef584345a6a0edd312d876b7f3",
            "28c0ffe2133244828ed5e5168c7f3195",
            "77e9b6c93ffd42a99d64f08be7bd7c22",
            "9617265be48d493780e086149c7874eb",
            "6b2bf860fb614b338e67cfb8ac6edfad",
            "4826c30f0456474bba3ab4c63cdf1e6d",
            "a89c79ac33d04c1fb2ee3b4258607e28",
            "24a0994484944067b7ca7eebdc7310a4",
            "b88eb239caa6493f8d8773c0339ebd53",
            "4750691c0e724269837822a39e97dfdc",
            "bc448c86bade4d22ba3adca0da33d416",
            "4165a459267040fbba4a7b920137c223",
            "13fe4edd95eb40c19da9955a23489243",
            "239aadec57544d30875a8aed8605e6cc",
            "e851614115ef4fd9ba27de57329e7dea",
            "3219cdbf74aa4c62a4f808ff2f0216e2",
            "273d60d5a6ad43d692e961d1696855a8",
            "177d7273825545c5b7d6477aa6852bd9",
            "0b0995f73d38406ca96db6676aeef04e",
            "99d87fc6bf8a4fb59d80b6d80da910f3",
            "c54f64b7cad44a1284edb6444de5f163",
            "634420b5ffcd43a491206c6d3795492d",
            "b7e045b4c8ea48b084c7f5a490881901",
            "9aba480fecbc499c9f41fd1f5b8c1726",
            "b76d4d752892409187e703f285b06cbd",
            "597ec59fbaef4dbca67fe9eae330152f",
            "a844f382b8204e72ac88051cf76a2d3d",
            "f982a25542f141be9420421709585959",
            "7ae6ab36587d48a38d9a94a1426eba49",
            "ad7d1b1d714d4fd7959331f61dfb24bd",
            "3f730eef1f954b6691be2546dcd3e5c5",
            "e0ebea3f1fea4fa08fbda6bcd9a9af87",
            "28413ea9a4b14db1b9475f356e723c62",
            "8bd3cd03a172406aa9a85122e948d170",
            "c5ebff29914748e4a09406356c5d7935",
            "5ef1cf44d2444e5d9cd8262d14119764"
          ]
        },
        "id": "aDGWGyY_tz8S",
        "outputId": "bb17e524-aabd-4685-ced9-4abaf1f7b945"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f763040c478a4a3f9ed428fd4552f0a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68148838f0dd481baa7271e576deade1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a951747edfb4f50aa8f7c067dc922b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80a15c461e6644008a0cb803d4362441"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "149b0c1e07774884beb1f7fd1de0c9ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce767e803ace40d3b0ba36aa05437389"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eba76aee53614c53ac2677cba563b92b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85acd388aae54b1691ceafa334dc6bc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9617265be48d493780e086149c7874eb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e851614115ef4fd9ba27de57329e7dea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "597ec59fbaef4dbca67fe9eae330152f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamanho do vetor de embedding: 384\n"
          ]
        }
      ],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "# Teste com um modelo menor, diferente do utilizado na versão final do chatbot\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "\n",
        "\n",
        "vector_test = embeddings.embed_query(\"Como criar uma Sessão de Julgamento no SEI Julgar?\")\n",
        "print(\"Tamanho do vetor de embedding:\", len(vector_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK-0sKgvuwtA"
      },
      "source": [
        "### 4. Construção da base vetorial (ChromaDB)\n",
        "\n",
        "Armazenamos os embeddings dos chunks em uma base vetorial persistente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUrSWTvguv-l",
        "outputId": "a42fc924-7579-4a42-85df-59d70a937523"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Banco vetorial criado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "import os\n",
        "\n",
        "persist_dir = \"./chroma_teste\"\n",
        "os.makedirs(persist_dir, exist_ok=True)\n",
        "\n",
        "vectorstore = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=persist_dir)\n",
        "\n",
        "print(\"Banco vetorial criado com sucesso!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FClS4N6vTPT"
      },
      "source": [
        "### 5. Testando o Recuperador (MMR)\n",
        "\n",
        "O recuperador MMR seleciona trechos relevantes evitando redundância. Ele equilibra diversidade e relevância, retornando apenas os melhores chunks para formar a resposta do modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl1585HAve8H",
        "outputId": "7a65aab7-6438-40a0-88e9-f4497ac65559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Documento 1 ---\n",
            "3.10 - Sessão de Julgamento Aberta O início da votação é condicionado a duas situações: 1. O Desembargador Relator deve disponibilizar o documento da sessão de julgamento juntamente com o provimento respectivo. 2. A presença do Desembargador Relator e do Presidente da Sessão de Julgamento é obrigatória. O provimento e complemento são indicados ao passar o mouse sobre o ícone . Quando o documento é\n",
            "\n",
            "--- Documento 2 ---\n",
            "Uma vez que a votação se encerra e a Secretaria do Colegiado finaliza a sessão, uma Certidão do Julgamento é anexada a todos os processos. O processo administrativo, por sua vez, permanece na unidade que o pautou, ao mesmo tempo que é automaticamente encaminhado à unidade do membro do colegiado que proferiu o voto vencedor . 2.8 - Sessão de Julgamento - Suspensa Somente a unidade da Secretaria pos\n",
            "\n",
            "--- Documento 3 ---\n",
            "edição são a inclusão de ressalvas aos votos e ajustes no dispositivo que constará da Certidão e da Ata. Após o encerramento da Sessão, os processos que estavam \"Em Julgamento\" passarão automaticamente para a situação \"Julgado\". As demais situações (retirado, adiado, convertido) permanecem inalteradas. Contudo, se houver membros do colegiado ausentes na Sessão de Julgamento, será obrigatório regis\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-507648491.py:7: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(query)\n"
          ]
        }
      ],
      "source": [
        "retriever = vectorstore.as_retriever(\n",
        "    search_type='mmr',\n",
        "    search_kwargs={'k': 3, 'fetch_k': 4, 'lambda_mult': 0.7}\n",
        ")\n",
        "\n",
        "query = \"Como iniciar uma votação da Sessão de Julgamento?\"\n",
        "docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "for i, d in enumerate(docs):\n",
        "    print(f\"--- Documento {i+1} ---\")\n",
        "    print(d.page_content[:400])\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYnMOYinvnf4"
      },
      "source": [
        "### 6. Função consolidada\n",
        "\n",
        "Depois de validar cada parte, montamos a função completa `config_retriver` para uso no chatbot.\n",
        "\n",
        "Leitura  →  Chunking  →  Embeddings  →  Banco vetorial  →  Recuperador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ-NOrY6yYe9"
      },
      "outputs": [],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from PyPDF2 import PdfReader\n",
        "import os\n",
        "\n",
        "def config_retriever():\n",
        "\n",
        "\n",
        "    # Cria o diretório onde o banco vetorial será armazenado\n",
        "    persist_dir = \"./chroma_db\"\n",
        "    os.makedirs(persist_dir, exist_ok=True)\n",
        "\n",
        "    # 1. Lê o PDF e divide o texto\n",
        "    pdf_file = \"Documentação SEI Julgar - Secretaria.pdf\"\n",
        "    reader = PdfReader(pdf_file)\n",
        "    texts = [page.extract_text() for page in reader.pages if page.extract_text()]\n",
        "\n",
        "    # 2.Segmentação (Chunking)\n",
        "    \"\"\"\n",
        "    Chunk_size - Define o tamanho dos chunks\n",
        "    Chunk_overlap - Define o quanto os chunks se sobrepõem\n",
        "    \"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "    chunks = splitter.create_documents(texts)\n",
        "\n",
        "    # 3. Geração de Embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
        "\n",
        "    # 4. Cria o banco vetorial com ChromaDB com os embeddings desses chunks\n",
        "    vectorstore = Chroma.from_documents(chunks, embedding=embeddings, persist_directory=persist_dir)\n",
        "\n",
        "    \"\"\"\n",
        "    O Maximal Marginal Relevance busca no banco os chunks mais relevantes e diversos para evitar a redundância de informações.\n",
        "    fetch_k - Define a quantidade de chunks que serão recuperados do banco\n",
        "    k - Define a quantidade de chunks que serão retornados a llm\n",
        "    lambda_mult - Controla o peso de Relevância e Diversidade 0.7(70% foco na relevância e 30% na diversidade)\n",
        "    \"\"\"\n",
        "    # 5. Recuperador MMR\n",
        "    return vectorstore.as_retriever(search_type='mmr', search_kwargs={'k': 3, 'fetch_k': 4, 'lambda_mult': 0.7})\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEzyduA8DFk2"
      },
      "source": [
        "---\n",
        "## **Geração (RAG Chain)**\n",
        "\n",
        "Depois que o retriever encontra os melhores trechos, o LLM utiliza essas informações para produzir uma resposta final, fiel ao conteúdo do manual.\n",
        "\n",
        "A seguir iremos ver todo o processo da RAG Chain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaqMavbcxWUP"
      },
      "source": [
        "### 1. Prompt para reformular a pergunta (context_prompt)\n",
        "\n",
        "Antes de consultar o banco vetorial, é importante que o modelo interprete a pergunta atual considerando o histórico da conversa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50Zdii6q74bX",
        "outputId": "3ccb7e38-f841-425c-bf14-871ef4c8b758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Reformule a pergunta do usuário, não responda.\n",
            "Human: Como registrar uma minuta?\n",
            "AI: Você pode usar a funcionalidade X do SEI Julgar.\n",
            "Human: e para assinar?\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "context_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Reformule a pergunta do usuário, não responda.\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"{input}\")\n",
        "])\n",
        "\n",
        "#Exemplo\n",
        "teste = context_prompt.format(\n",
        "    chat_history=[\n",
        "        (\"human\", \"Como registrar uma minuta?\"),\n",
        "        (\"ai\", \"Você pode usar a funcionalidade X do SEI Julgar.\")\n",
        "    ],\n",
        "    input=\"e para assinar?\"\n",
        ")\n",
        "\n",
        "print(teste)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daN01Cne7-uA"
      },
      "source": [
        "### 2. Criando o History-Aware Retriever\n",
        "\n",
        "O History-Aware Retriever utiliza o prompt de reformulação para transformar perguntas vagas em consultas claras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n43Sc6dAC4cW",
        "outputId": "f847c937-faa6-44a7-a2d0-2c7f11dac17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pergunta reformulada:\n",
            "Você gostaria de saber como estabelecer e iniciar uma Sessão de Julgamento, incluindo os passos necessários para criá-la e iniciá-la?\n"
          ]
        }
      ],
      "source": [
        "# Simula o que o history-aware retriever faria internamente:\n",
        "reformulada = llm.invoke(\n",
        "    context_prompt.format(\n",
        "        chat_history=[(\"human\", \"Como criar a Sessão de Julgamento?\")],\n",
        "        input=\"e como eu abro a Sessão?\"\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Pergunta reformulada:\")\n",
        "print(reformulada.content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPrHRzov5-7Q"
      },
      "source": [
        "### 3. Prompt principal de QA (qa_prompt)\n",
        "\n",
        "Este prompt define como o chatbot deve responder.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m20oDFwLB9pV",
        "outputId": "3a538a80-d344-4176-e442-c88e9722a713"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "System: Você é um assistente especialista no SEI Julgar.\n",
            "Human: Como registrar uma minuta?\n",
            "Human: Pergunta: Como iniciar uma votação?\n",
            "\n",
            "Contexto do manual:\n",
            "Trecho simulado do manual: Para iniciar uma votação...\n"
          ]
        }
      ],
      "source": [
        "qa_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Você é um assistente especialista no SEI Julgar.\"),\n",
        "    MessagesPlaceholder(\"chat_history\"),\n",
        "    (\"human\", \"Pergunta: {input}\\n\\nContexto do manual:\\n{context}\")\n",
        "])\n",
        "\n",
        "teste = qa_prompt.format(\n",
        "    chat_history=[(\"human\", \"Como registrar uma minuta?\")],\n",
        "    input=\"Como iniciar uma votação?\",\n",
        "    context=\"Trecho simulado do manual: Para iniciar uma votação...\"\n",
        ")\n",
        "\n",
        "print(teste)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efjAHrTGE7CE"
      },
      "source": [
        "### 5. Função consolidada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PH48mHpoNQt3"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "#Cria pipeline RAG: reformulação + busca + resposta final.\n",
        "def config_rag_chain(llm, retriever):\n",
        "\n",
        "    # Prompt que reformula perguntas do usuário\n",
        "    context_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"Reformule perguntas do usuário sem respondê-las.\"),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\")\n",
        "    ])\n",
        "\n",
        "    # Recuperador com histórico\n",
        "    history_retriever = create_history_aware_retriever(\n",
        "        llm=llm, retriever=retriever, prompt=context_prompt\n",
        "    )\n",
        "\n",
        "    # Prompt principal de QA\n",
        "    qa_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\", \"\"\"\n",
        "        Você é um assistente virtual especializado **exclusivamente** no módulo **SEI Julgar**.\n",
        "        Forneça respostas **técnicas e objetivas**, baseadas apenas na documentação oficial.\n",
        "\n",
        "        REGRAS:\n",
        "        1. Não inventar informações.\n",
        "        2. Não responder temas fora do SEI Julgar.\n",
        "        3. Manter tom formal e instrutivo.\n",
        "        4. Se o tema for irrelevante, diga: \"Posso responder apenas sobre o funcionamento do SEI Julgar.\"\n",
        "        \"\"\"),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"Pergunta: {input}\\n\\nContexto relevante do manual:\\n{context}\")\n",
        "    ])\n",
        "\n",
        "    #envia a llm o prompt acima mais os documentos retornados pelo retriever\n",
        "    qa_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "    return create_retrieval_chain(history_retriever, qa_chain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYYd4PLkEuhr"
      },
      "source": [
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uclGn9uZrlp"
      },
      "source": [
        "---\n",
        "## **Demonstração (Retriever + RAG)**\n",
        "\n",
        "Agora que vimos separadamente como funcionam o Retriever e o RAG, vamos fazer um teste simples integrando todas as etapas.\n",
        "\n",
        "Usuário → Retriever → RAG → Resposta."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "depTThGIbHcR"
      },
      "source": [
        "#### 1. Criando um documento qualquer\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGjIxOBHa29w"
      },
      "outputs": [],
      "source": [
        "docs = [\n",
        "    \"O SEI Julgar é um módulo responsável pela condução de sessões de julgamento e votações colegiadas.\",\n",
        "\n",
        "    \"Para iniciar a votação de um processo, o membro deve acessar o painel de votação correspondente. \"\n",
        "    \"Entretanto, a Secretaria do Colegiado é responsável por liberar a votação para que ela possa ser iniciada.\",\n",
        "\n",
        "    \"A Secretaria do Colegiado pode fechar a pauta, o que impede alterações pelos gabinetes dos desembargadores.\",\n",
        "\n",
        "    \"É possível acompanhar o andamento da votação na aba Resultados, onde ficam visíveis os votos já registrados.\",\n",
        "\n",
        "    \"A votação pode ser encerrada manualmente ou automaticamente conforme as regras configuradas na sessão.\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQWrGABybNsh"
      },
      "source": [
        "#### 2. Criando a base vetorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456,
          "referenced_widgets": [
            "e7cf66c201804315a3e2541ae84e86d6",
            "eaa2ea8dc75743fda202814a71eaf755",
            "4f9605d5e2fb446fa6827d5c47c620eb",
            "160e8b651c16422ca1fb0103a5820ddf",
            "6a452017cea142fe8055cfb2532527f2",
            "339d0a390e2543afa8914a940f26dd2d",
            "a7e8e8e638e343acb3a8486a5b187ca8",
            "dd5f708b22b54f06bc0ab374367381ac",
            "40c4aecf7ea34bcca5ab198bd7a69ad7",
            "06278daeb1cd41efbddd7abaf82862ad",
            "480da1d62acb404aa902ea59b7d56503",
            "dbe739e9550745528b74507d5e6d61f8",
            "c15b3a4e3f89412dac6da437eef4c190",
            "752cbb8271d14289b1b106347c4ff2f3",
            "f96b8d36ed374a7f8896bebafa703bfc",
            "f4232a0f951c4125873d67587f5c36ef",
            "7a2ee87d9e5f4b4ca222af221217c654",
            "a88c7742df9c4ddeb418b26ea43013a8",
            "4ec67a5c073d4560b990acf6cc2f0e7a",
            "4a85d263f12e4ece9a266c77d023fd80",
            "ebbfe8682f704bc4b6e8e8489564c135",
            "051dcc5dfc0c4ab884ac67c91820bd6e",
            "b34838ff0b3840c6ac24c26839254665",
            "9e8810ff5fb0479e98867cc961acabe7",
            "7fdd01cb0eca445dbb7a425df40c346d",
            "8df314dede5c4b3fb9bf3fccc704c6b7",
            "10bae1dd8f8d41c9b1a4cbc2fa86376a",
            "4613047500b042e1912f1566313547c9",
            "baef8f8a8c3f453eae13ad9a608a815d",
            "916febfdcc674731bf6624c5e8cc2428",
            "640db551d6c142c1adb5adf4f65747aa",
            "946d7a6140924f37adf82d06aeeef396",
            "dacf45fa9d0e4604a55ebe599b350857",
            "c4444f9694154d0aa186e184c0b52616",
            "89e50ef13c554bd39803f203a9631de2",
            "cf017625c4944627a6bebe44cc123feb",
            "9d183430880f495fab6601ff8fc40bed",
            "46c8c403be2149309e80c648c8aec22a",
            "da5f3aa927ca4d3e88effd2bd631966e",
            "6347107661f149c387321a05abc6a93b",
            "c41b63ea172b4042ae859e4c03db1d0a",
            "828792c872c942b4b62d975c74526cef",
            "5249ca3be77f4390967eaa33a4fc3164",
            "d4397e03c0b146968d81d37fbe57badb",
            "f9275a3e302942c79006f13df2df1a4f",
            "d3e31b63fc6847a2be78202cb744e775",
            "43e4ed30b902462e8f071f36734e4c68",
            "dda9a0630f5c4cc5ab18d613b131cf43",
            "818aaff405cb464682fc323488f45728",
            "2efe1d26c86b489d8aeeb584f3edef73",
            "334dae3760484c8e94f429d7c725d3d6",
            "ea1fd0bde7c9443fa417092a45856e15",
            "650bf8ecd4a5402baa787e88fefc9e71",
            "15c9cb78eb504d578ac655f5170c651e",
            "0ce6564a927c47d393a6052f8aa88eb4",
            "d040ff5e8a72402f9ee2574b83c55b82",
            "881364c578a14e4f905762cb25da067c",
            "a2b9746320b1416b9fe3605924f85d4a",
            "6ea73a85737f42468618226ab45d32a8",
            "c83888078a4f421db88d249361be2c9a",
            "73457cea07e144219b25c65dde88ce79",
            "eaa6003bbaf94750a6b388a425b0036d",
            "2cd8aceb90a94f4e834a6b144c30eedf",
            "f8fb2d1524ac48babdbd76e16fd18b05",
            "e879e32ce96f447f81b57cec2521c560",
            "3e12a447c1bc455ba12b0cc8753f5c2e",
            "c5782a50f10a46b2bb149b053b3d9ef7",
            "6c9c4377f1b040febc8e12375138f9ea",
            "b6540a26d28a429e9ecdcd1fa043b27d",
            "4e3cacf371b74b9aa6c1fa08bcee6741",
            "4ee63a3d95a947599c2018e0951ac495",
            "a0c0ae30de9949208a995d1457075cc3",
            "7b08b337d6da4e0f94db100670e07e65",
            "95c266e18e0b4d68907c21b45ba6882b",
            "326b4ac452b54bf986c7cb89b45fe079",
            "ce45f4b44e844069b9312c4a0fa90b1f",
            "ba2a6489bd2f4243bb99da21abe7e34e",
            "434121f1c72447d0b827b770659ea103",
            "330ba899305841aa91daad53e95046f2",
            "f9419c6d55b54585b7ba23bc4cf9d8e8",
            "5c839f236bd644eba14881c138cb4832",
            "c08abf3dc6b1421fbd7e0a249ffa6796",
            "449f5403b2114bbd9f6eb94b2d396964",
            "0f8dbb35f0bf44b2a451d04c8edd113c",
            "2971ffe3ce5449159cc260c8c552236f",
            "89f322a4e2fe442b9f75ae7409023a5f",
            "47e15ebad14e4b5fa372b0ca614624d4",
            "967b7e06980d42708b81e98264809c17",
            "f842eed04642442aa6f76002058b1b34",
            "4791e5c83a4a4868998ee92d8f364d45",
            "666d2de3bb6e4731ba4d39a46d9a0146",
            "145a2f3ca19d4756b78a31da5bf884ba",
            "00d3239f486a453caf167d316e4009e0",
            "23df97ae15cd425fbd7815ca8822c9e4",
            "033d44f669ae4f3e9943db6401be3fe5",
            "9be1fd19ee4c4792911e5987339e1276",
            "7e206b88558f48068ae57af6d3a6b738",
            "c1e33d97aabe4d90a36ed1a22ea52195",
            "f054b4024afd4efb8c20bc82802e2051",
            "47f5080d8d7040099bf5bdbff282c3c2",
            "42b8423265e344e5b219863761fe83d1",
            "b5a62536d38f4c9780ce2956c5cc8767",
            "87a60a9d55d64db5b5a39c1d4e78ba4a",
            "8419b7abe6274eccae7b1771a5686876",
            "14a7fc332e4b4eb6b9759b7a248dc474",
            "c3c91892a1f342d886b0ab5e38b8b305",
            "07873b8f28dd490cb2ba8c89cd461331",
            "a12c3b911fab42a8a603c827438717f3",
            "3d8dd97feee142c0a9a03db04a1bd874",
            "01e74f65d0454ff1ae6e5f9424c6cada",
            "3b5d123ec97d4831b583e5839532f96e",
            "2069c04cbf4f42ac958440cc4dc9cb46",
            "c289026d9ac14d07aac74633e30a5750",
            "176f1a1daa544c8fa0ace5c7a40ef0fa",
            "e8a2eb853bb84524ab98e92e57b3d25a",
            "398c5e00992d4f8ba4f90a8a91c8caa4",
            "518d39839e394d339f180fda8d38d2d1",
            "4b2dde19f9204f3b847f60887b67ee4e",
            "09353e0ba4804974a3e7dbbd3c45f43b",
            "5d18872c93a047ff9f6577afe67379e9",
            "15fb03b1d0ec458f851b7a19409246da",
            "2158b6605d21455399634d71498c9186",
            "e5c14b0c8c8f4b6781ed0e03a12f0226",
            "ecad7f5c832b479ebeb31152eaa23083",
            "3b1f038a0ff94da898bddb81c9351f01",
            "19a91c3a7d4a4dd9a43ac4e5064e0b2d",
            "05fe61af498d4781945535bf80e005ad",
            "446dfb1b02bf441ca19e4f6ad063a90c",
            "0cdde5c4ebc144c1bd5207b6d94f0325",
            "86f89fe41858463da872e3f41caeb3e1",
            "af93fcc7f4e8427eadb0744e8df96f7e",
            "f5e43592cb9f491a9b8f381e735c9c21"
          ]
        },
        "id": "IrDU_MygbUpZ",
        "outputId": "f25933c3-3a64-43cb-a8af-d12d3b5a4a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4232430324.py:13: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7cf66c201804315a3e2541ae84e86d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/123 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbe739e9550745528b74507d5e6d61f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b34838ff0b3840c6ac24c26839254665"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4444f9694154d0aa186e184c0b52616"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/687 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9275a3e302942c79006f13df2df1a4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d040ff5e8a72402f9ee2574b83c55b82"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.27G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5782a50f10a46b2bb149b053b3d9ef7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/444 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "434121f1c72447d0b827b770659ea103"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f842eed04642442aa6f76002058b1b34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47f5080d8d7040099bf5bdbff282c3c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b5d123ec97d4831b583e5839532f96e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/191 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2158b6605d21455399634d71498c9186"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import os, shutil\n",
        "\n",
        "# cria base temporária\n",
        "persist_dir = \"./chroma_demo\"\n",
        "os.makedirs(persist_dir, exist_ok=True)\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
        "chunks = splitter.create_documents(docs)\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-m3\")\n",
        "\n",
        "vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=persist_dir)\n",
        "retriever_demo = vectorstore.as_retriever()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlUFLRi9bboh"
      },
      "source": [
        "#### 3. RAG Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E478XbOkbexp"
      },
      "outputs": [],
      "source": [
        "rag_demo = config_rag_chain(llm, retriever_demo)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hco5pZBXbhDp"
      },
      "source": [
        "#### 4. Teste final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFpfSPpRWarj",
        "outputId": "2e875f0e-8a86-4643-8db4-74dd7e54d94e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Resposta Final:\n",
            "Para iniciar uma votação no SEI Julgar, é necessário que haja um documento disponibilizado no processo. Portanto, o primeiro passo é garantir que esse requisito seja atendido. Uma vez que o documento esteja disponível, a votação pode ser iniciada, considerando as funcionalidades e restrições do sistema, como a manipulação dos votos sendo uma função restrita à secretaria.\n",
            "\n",
            " Query reformulada:\n",
            "Como iniciar uma votação?\n",
            "\n",
            " 3 trechos usados:\n",
            "\n",
            "--- Trecho 1 ---\n",
            "uma condição obrigatória que haja um documento disponibilizado no processo para que a votação possa ser iniciada. 6...\n",
            "\n",
            "\n",
            "--- Trecho 2 ---\n",
            "para alterar o provimento de um voto já registrado, mesmo o do Relator . A manipulação dos votos da Sessão é uma função restrita à secretaria; todos os outros usuários estão limitados à visualização dos dados. 13...\n",
            "\n",
            "\n",
            "--- Trecho 3 ---\n",
            "Desembargador presente. O destaque não remete à votação em si, ou seja, um(a) Desembargador(a) ou a Secretaria pode selecionar um determinado destaque, mas no momento da votação registrar um voto diferente do destaque selecionado. 8...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1 Criar a RAG Chain\n",
        "rag_chain = config_rag_chain(llm, retriever)\n",
        "\n",
        "\n",
        "# 2 Pergunta de demonstração\n",
        "user_question = \"Como iniciar uma votação?\"\n",
        "\n",
        "input_text = user_question\n",
        "\n",
        "# 3 Executa o RAG\n",
        "response = rag_chain.invoke({\n",
        "    \"input\": input_text,\n",
        "    \"chat_history\": []\n",
        "})\n",
        "\n",
        "\n",
        "# 4 Exibir resultados\n",
        "print(\"\\n Resposta Final:\")\n",
        "if isinstance(response, dict):\n",
        "    print(response.get(\"answer\", \"Nenhuma resposta encontrada.\"))\n",
        "\n",
        "    if \"input\" in response:\n",
        "        print(\"\\n Query reformulada:\")\n",
        "        print(response[\"input\"])\n",
        "\n",
        "    if \"context\" in response:\n",
        "        print(f\"\\n {len(response['context'])} trechos usados:\")\n",
        "        for i, d in enumerate(response[\"context\"][:3], 1):\n",
        "            content = getattr(d, \"page_content\", str(d))\n",
        "            print(f\"\\n--- Trecho {i} ---\\n{content[:350]}...\\n\")\n",
        "\n",
        "else:\n",
        "    print(\"Retorno inesperado:\", response)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5NBJ9XQbtZo"
      },
      "outputs": [],
      "source": [
        "# Caso queira limpar a base de teste\n",
        "shutil.rmtree(\"./chroma_demo\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zt9xDjp-7WpX"
      },
      "source": [
        "\n",
        "\n",
        "Até aqui percorremos todas as etapas essenciais para a construção do chatbot.\n",
        "Agora, usaremos o Streamlit para demonstrar a versão final do chatbot do SEI Julgar, pois é uma ferramenta eficiente e interativa para desenvolver interfaces web em Python e utilizar CSS personalizado.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sS2UcK1WPZ9A"
      },
      "source": [
        "### Chave Groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCm3qxFKIdF3",
        "outputId": "b5436b84-82c2-4bd6-9db8-c230006c361c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing .env\n"
          ]
        }
      ],
      "source": [
        "%%writefile .env\n",
        "GROQ_API_KEY=coloque_sua_chave_aqui"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgr97JuX4gJ-"
      },
      "source": [
        "### Importe o manual Documentação SEI Julgar (caso não tenha feito anteriormente) e arquivo chatbot_sei.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "-xbxI8rl4WV6",
        "outputId": "ae132b1d-e9e3-490e-9e54-8482a82864a3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aee569ad-b778-41a9-8310-cdeafa9f2b3d\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aee569ad-b778-41a9-8310-cdeafa9f2b3d\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving chatbot_sei.py to chatbot_sei.py\n",
            "Arquivo enviado: chatbot_sei.py\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for filename in uploaded.keys():\n",
        "    print(f\"Arquivo enviado: {filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzcD6Wb137qB"
      },
      "source": [
        "Após executar o código a seguir, clique no link em azul https://teams-rough-maximize... que irá aparecer no log da execução.\n",
        "\n",
        "Demora um pouco para base ser criada e o modelo ser carregado, apenas na primeira execução."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJX9TPqZv1R1",
        "outputId": "8630d391-735e-4110-93ba-7384c1a47600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2025-11-22T22:16:01Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2025-11-22T22:16:01Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m |  https://medal-numeric-buyers-covering.trycloudflare.com                                   |\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 991dffd8889ee9f0147b6b48933da9e4407e68ea8c6d984f55fa2d3db4bb431d)\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: e82c22ea-eb52-43d7-9b31-f78a70e51df4\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7\n",
            "2025/11/22 22:16:04 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2025-11-22T22:16:04Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0mda70b373-b377-45e0-bcc6-8e88addc2d1f \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.7 \u001b[36mlocation=\u001b[0mord10 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2025-11-22T22:27:16Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "# 1️⃣ Instalar Streamlit e Cloudflared\n",
        "!pip install streamlit -q\n",
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n",
        "!mv cloudflared-linux-amd64 /usr/local/bin/cloudflared\n",
        "\n",
        "# 2️⃣ Rodar Streamlit em background\n",
        "!streamlit run /content/chatbot_sei.py &>/content/logs.txt &\n",
        "\n",
        "# 3️⃣ Expor via Cloudflared\n",
        "!cloudflared tunnel --url http://localhost:8501\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBG0Hdy9PvKb"
      },
      "source": [
        "### Caso queira excluir a base vetorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoptkpxvA4ms"
      },
      "outputs": [],
      "source": [
        "rm -r chroma_db\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68U51slkIi3W"
      },
      "source": [
        "---\n",
        "## **Conclusão**\n",
        "O projeto obteve êxito na criação do Assistente Virtual do SEI Julgar utilizando a arquitetura RAG. Nosso foco principal foi tornar a consulta ao manual muito mais ágil e intuitiva, seguindo as etapas detalhadas neste notebook.\n",
        "\n",
        "Com essa implementação, garantimos que a Inteligência Artificial forneça respostas técnicas e diretas, baseadas estritamente no manual do SEI Julgar. Isso é essencial para eliminar alucinações por parte da LLM e oferecer a precisão e segurança que as secretarias e gabinetes dos Magistrados necessitam.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}